import torch
import torch.nn.functional as F
from skimage.filters import gaussian


def post_process_output(q_img, cos_img, sin_img, width_img):
    """
    Post-process the raw output of the network, convert to numpy arrays, apply filtering.
    :param q_img: Q output of network (as torch Tensors)
    :param cos_img: cos output of network
    :param sin_img: sin output of network
    :param width_img: Width output of network
    :return: Filtered Q output, Filtered Angle output, Filtered Width output
    """
    q_img = q_img.cpu().numpy().squeeze()
    ang_img = (torch.atan2(sin_img, cos_img) / 2.0).cpu().numpy().squeeze()
    width_img = width_img.cpu().numpy().squeeze() * 150.0

    q_img = gaussian(q_img, 2.0, preserve_range=True)
    ang_img = gaussian(ang_img, 2.0, preserve_range=True)
    width_img = gaussian(width_img, 1.0, preserve_range=True)

    return q_img, ang_img, width_img


def post_process_output2(theta, width, q_val):

    theta = theta.cpu().numpy().squeeze()
    width = width.cpu().numpy().squeeze()
    q_val = q_val.cpu().numpy().squeeze()

    return theta, width, q_val


def post_process_output3(output):
    """
    Post-process the raw output of the network, convert to numpy arrays, apply filtering.
    :param output: 10 vaild grasp samples generated by model 
    """
    output = output.cpu().numpy().squeeze()
    
    return output


def regress_grasp_pose(net, xc, grid_w, grid_a, n_candidates=(2, 1), thresh_conf=0.1):
    """
    This function implemented a coarse-to-fine prediction. (used for evaluation)
    Args:
        net: the pretrained model
        xc: input data including RGB-D input and gripper input
        grid_w: discretized width space, used for search the output width
        grid_a: discretized angle space, used for search the output angle
        n_candidates: number of candidate outputs
        thresh_conf: confidence threshold used for output selection

    Returns:
        a (N, 3) matrix representing the output width, angle, and quality (confidence)

    """
    gripper_inp, scene_inp = xc
    device = scene_inp.device
    grid_w, grid_a = grid_w.squeeze(0), grid_a.squeeze(0)
    nw, na = grid_w.shape[0], grid_w.shape[1]

    """B, H, W = gripper_inp.shape[0], gripper_inp.shape[-2], gripper_inp.shape[-1]
    out_prob = F.sigmoid(net(scene_inp, gripper_inp.reshape(B, -1, H, W)))  # [B, Nc]
    out_prob = out_prob.squeeze(0)  # batch_size should be 1

    out_confs, ind_max = torch.max(out_prob, dim=0)
    out_confs = out_confs.unsqueeze(0)
    out_widths = grid_w.reshape(-1)[ind_max].unsqueeze(0)
    out_angles = grid_a.reshape(-1)[ind_max].unsqueeze(0)"""

    c_grid_idw, c_grid_ida = torch.meshgrid(torch.arange(0, nw//4, 1, device=device),
                                            torch.arange(0, na//4, 1, device=device))
    c_gripper_inp = gripper_inp[:, c_grid_idw.reshape(-1)*4+2, c_grid_ida.reshape(-1)*4+2, :, :]  # [B, Nc, H, W]

    out_prob = F.sigmoid(net(scene_inp, c_gripper_inp))  # [B, Nc]
    out_prob = out_prob.squeeze(0)  # batch_size should be 1

    #if torch.count_nonzero((out_prob > thresh_conf)) == 0:
    #    return torch.zeros((10, 3), dtype=torch.float32, device=device)
    #else:
    out_confs, inds = torch.topk(out_prob, k=n_candidates[0])

    grid_idw, grid_ida = torch.meshgrid(torch.arange(0, nw, 1, device=device, dtype=torch.float32),
                                            torch.arange(0, na, 1, device=device, dtype=torch.float32))
    grid_idw_unfold = F.unfold(grid_idw.unsqueeze(0).unsqueeze(0), kernel_size=4, stride=4).squeeze(0).t()
    grid_ida_unfold = F.unfold(grid_ida.unsqueeze(0).unsqueeze(0), kernel_size=4, stride=4).squeeze(0).t()

    f_grid_idw, f_grid_ida = grid_idw_unfold[inds].long().reshape(-1), grid_ida_unfold[inds].long().reshape(-1)
    f_gripper_inp = gripper_inp[:, f_grid_idw, f_grid_ida, :, :]
        # xf = torch.cat((scene_inp.repeat(f_gripper.shape[0], 1, 1, 1), f_gripper.unsqueeze(1)), dim=1)
    out_prob = F.sigmoid(net(scene_inp, f_gripper_inp))
    out_prob = out_prob.squeeze(0)

    out_confs, inds = torch.topk(out_prob, k=n_candidates[1])
    out_widths = grid_w[f_grid_idw, f_grid_ida][inds]
    out_angles = grid_a[f_grid_idw, f_grid_ida][inds]
    return torch.stack((out_angles, out_widths, out_confs), dim=-1)